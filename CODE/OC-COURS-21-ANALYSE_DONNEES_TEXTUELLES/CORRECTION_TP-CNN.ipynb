{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activité 1 - Analysez vos données textuelles\n",
    "\n",
    "NB : J'ai dû lancer jupyter avec la commande suivante :<br>\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "Ceci pour éviter d'avoir l'erreur décrite ici: <br>\n",
    "https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image\n",
    "\n",
    "\n",
    "### Objectif\n",
    "Le but de cette activité est de nettoyer les données textuelles brutes fournies, et de créer un jeu de données d’entraînement en vue de créer un moteur de résumé automatique. \n",
    "\n",
    "Le jeu de données est téléchargeable ici: https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\n",
    "\n",
    "\n",
    "### Contexte\n",
    "Les données brutes représentent un corpus d’articles CNN. L’objectif est de récupérer les features des documents et les highlights (résumés courts) associés concaténés, en vue d’entraîner un potentiel modèle de création de résumé d’articles.\n",
    "\n",
    "### Consigne\n",
    "Les opérations de traitement suivantes sont attendues sur le texte, pas forcément dans cet ordre :\n",
    "* Créer des paires de document (article, highlights)\n",
    "* Suppression de la ponctuation\n",
    "* Séparation en token en minuscules\n",
    "* Suppression des stopwords pour les articles\n",
    "* Calcul des fréquences et tf-idf sur les deux types de documents\n",
    "* Enregistrement du nouveau jeu de données d’entraînement pour usage ultérieur\n",
    "\n",
    "### Etapes\n",
    "#### 1. Lecture des données et création de paires (article, highlights).\n",
    "* On passe le texte en minuscules dès cette étape.\n",
    "   \n",
    "   \n",
    "#### 2. Tokenization des articles et des highlights.\n",
    "* Les stopwords sont conservés pour les highlights, mais retirés pour les articles. \n",
    "* Les deux corpus tokenizés sont stockés dans des listes.    \n",
    "* L'utilisation d'un RegexpTokenizer permet d'obtenir des tokens sans ponctuation.\n",
    "   \n",
    "   \n",
    "#### 3. Calcul des fréquences et tf-idf sur les deux types de documents.\n",
    "* Création de deux matrices de fréquences, une pour les articles et une pour les highlights, dénombrant le nombres d'occurrences de chaque mot dans chaque document.\n",
    "* Création de deux matrices tf-idf, une pour les articles et une pour les highlights, stockant le tf-idf de chaque paire (document, mot). \n",
    "* Création de deux dictionnaires, un pour les articles et un pour les highlights, permettant de mapper les mots dénombrés dans les matrices ci-dessus à leur représentation sous forme d'entier. \n",
    "   \n",
    "   \n",
    "#### 4. Stockage des listes et dictionnaires créés dans un fichier pickle. \n",
    "* Stockage des listes tokenizées, matrices de fréquence et tf-idf dans un fichier pickle.\n",
    "* On les recharge juste après pour vérifier que tout fonctionne. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Lecture des données et création de paires (article, highlights).\n",
    "* Lecture du jeu d'entraînement.\n",
    "* Création de deux listes, *articles* et *highlights*, contenant respectivement les articles et les résumés des fichiers du dataset passés en minuscules.\n",
    "* Création d'une liste *pairs* contenant des tuples (article, highlight), constitués à partir des index de chaque texte dans les listes *articles* et *highlights*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_dataset(directory, articles, highlights, pairs):\n",
    "    \"\"\"\n",
    "    Browse the directory containing the dataset files.\n",
    "    Load each file content in memory. \n",
    "    Replace all upper case letters with lower case letters.\n",
    "    Add: \n",
    "      - The initial text in lower case to the articles list\n",
    "      - The highlights to the highlights list\n",
    "      - The pair of indexes to the pairs list\n",
    "    \"\"\"\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        \n",
    "        with open(directory + '/' + entry, 'r', encoding=\"utf8\") as file:\n",
    "            \n",
    "            observation = file.read().lower()\n",
    "            splitted = observation.split(\"@highlight\")\n",
    "            articles.append(splitted[0])\n",
    "            article_index = len(articles) - 1\n",
    "            \n",
    "            for highlight in splitted[1:]:\n",
    "                highlights.append(highlight)\n",
    "                highlight_index = len(highlights) - 1\n",
    "                pairs.append((article_index, highlight_index))           \n",
    "                    \n",
    "articles = []\n",
    "highlights = []\n",
    "pairs = []\n",
    "load_dataset('cnn/stories', articles, highlights, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Tokenization des articles et des highlights.\n",
    "* Les stopwords sont conservés pour les highlights, mais retirés pour les articles. \n",
    "* Les deux corpus tokenizés sont stockés dans des listes: *tokenized_articles* et *tokenized_highlights*    \n",
    "* L'utilisation d'un RegexpTokenizer permet d'obtenir des tokens sans ponctuation.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    " \n",
    "def preprocess(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Suppress all punctuation in the given text. \n",
    "    Remove the stopwords if remove_stopwords is True.\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    \n",
    "    if remove_stopwords is True:\n",
    "        tokenized = [w for w in tokenized if w not in stop_words]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "tokenized_articles = [preprocess(a, remove_stopwords=True) for a in articles]\n",
    "tokenized_highlights = [preprocess(h) for h in highlights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Calcul des fréquences et tf-idf sur les deux types de documents.\n",
    "* Création de deux matrices de fréquences, *counts_articles* et *counts_highlights*, dénombrant le nombres d'occurrences de chaque mot dans chaque document.\n",
    "* Création de deux matrices tf-idf, *tfidf_articles* et *tfidf_highlights*, stockant le tf-idf de chaque paire (document, mot). \n",
    "* Création de deux dictionnaires, *vocabulary_articles* et *vocabulary_highlights*, permettant de mapper les mots dénombrés dans les matrices ci-dessus à leur représentation sous forme d'entier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def find_frequency(text_list, vectorizer):\n",
    "    \"\"\"\n",
    "    Apply the given vectorizer to the text_list.\n",
    "    \"\"\"\n",
    "    # Remove punctuation.\n",
    "    punctuation = re.compile('\\W+')\n",
    "    text_list = [punctuation.sub(r' ', text) for text in text_list]\n",
    "    \n",
    "    # Create a dictionary containing the count of each word of the vocabulary.\n",
    "    word_count_vector = vectorizer.fit_transform(text_list)\n",
    "    return word_count_vector\n",
    "\n",
    "# Find the count of each word in the articles and highlights corpuses\n",
    "cv_articles = CountVectorizer(stop_words=stop_words)\n",
    "cv_highlights = CountVectorizer()\n",
    "\n",
    "counts_articles = find_frequency(articles, cv_articles)\n",
    "counts_highlights = find_frequency(highlights, cv_highlights)\n",
    "\n",
    "# Find the tfidf for both the articles and highlights corpuses\n",
    "tv_articles = TfidfVectorizer(stop_words=stop_words)\n",
    "tv_highlights = TfidfVectorizer()\n",
    "\n",
    "tfidf_articles = find_frequency(articles, tv_articles)\n",
    "tfidf_highlights = find_frequency(highlights, tv_highlights)\n",
    "\n",
    "# Find the vocabulary dictionaries for each corpus\n",
    "vocabulary_articles = cv_articles.vocabulary_\n",
    "vocabulary_highlights = cv_highlights.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Stockage des listes et dictionnaires créés dans un fichier pickle. \n",
    "* Stockage des listes tokenizées, matrices de fréquence et tf-idf dans un fichier pickle.\n",
    "* On les recharge juste après pour vérifier que tout fonctionne. \n",
    "* On affiche un extrait de la matrice *counts_articles*, contenant, pour chaque couple (*document*, *word*), le nombre d'occurrences du mot *word* dans le document *document*.\n",
    "* On affiche un extrait de la matrice *tfidf_highlights*, contenant, pour chaque couple (*document*, *word*), le tf-idf du mot *word* dans le document *document*.\n",
    "* On vérifie sur un exemple que les paires (article, highlight) sont cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dataset to a pickle file\n",
    "with open('data_activite1.pickle', 'wb') as pickle_file:\n",
    "    \n",
    "    pickle.dump(pairs, pickle_file)\n",
    "    \n",
    "    pickle.dump(tokenized_articles, pickle_file)\n",
    "    pickle.dump(tokenized_highlights, pickle_file)\n",
    "    \n",
    "    pickle.dump(vocabulary_articles, pickle_file)\n",
    "    pickle.dump(vocabulary_highlights, pickle_file)\n",
    "    \n",
    "    pickle.dump(counts_articles, pickle_file)\n",
    "    pickle.dump(counts_highlights, pickle_file)\n",
    "    \n",
    "    pickle.dump(tfidf_articles, pickle_file)\n",
    "    pickle.dump(tfidf_highlights, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dumped elements are loaded back into memory to check they were properly stored.\n",
    "with open('data_activite1.pickle', 'rb') as pickle_file:\n",
    "       \n",
    "    pairs = pickle.load(pickle_file)\n",
    "    \n",
    "    tokenized_articles = pickle.load(pickle_file)\n",
    "    tokenized_highlights = pickle.load(pickle_file)\n",
    "    \n",
    "    vocabulary_articles = pickle.load(pickle_file)\n",
    "    vocabulary_highlights = pickle.load(pickle_file)\n",
    "    \n",
    "    counts_articles = pickle.load(pickle_file)\n",
    "    counts_highlights = pickle.load(pickle_file)\n",
    "    \n",
    "    tfidf_articles = pickle.load(pickle_file)\n",
    "    tfidf_highlights = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SANITY CHECK 1\n",
      "\n",
      "Here are 10 tuples (A, W) with the number of times the word W appears in the article A:\n",
      "     The article 39244  contains 4   time(s) the word ohio    \n",
      "     The article 20728  contains 1   time(s) the word hour    \n",
      "     The article 2074   contains 1   time(s) the word able    \n",
      "     The article 61799  contains 7   time(s) the word prix    \n",
      "     The article 67135  contains 1   time(s) the word oldest  \n",
      "     The article 62691  contains 1   time(s) the word 20      \n",
      "     The article 24711  contains 3   time(s) the word pun     \n",
      "     The article 55075  contains 3   time(s) the word bush    \n",
      "     The article 25631  contains 1   time(s) the word surface \n",
      "     The article 71987  contains 1   time(s) the word national\n",
      "\n",
      "SANITY CHECK 2\n",
      "\n",
      "Here are 10 tuples (H, W) with the tfidf of the word W in the highlight H:\n",
      "     The word kill            in the highlight 142594   has a tf-idf of 0.25769197225374746.\n",
      "     The word criticism       in the highlight 239344   has a tf-idf of 0.3169361958033549.\n",
      "     The word maritime        in the highlight 325837   has a tf-idf of 0.3924885440319226.\n",
      "     The word for             in the highlight 122992   has a tf-idf of 0.12082530217260704.\n",
      "     The word in              in the highlight 47200    has a tf-idf of 0.08429504176246533.\n",
      "     The word lech            in the highlight 283422   has a tf-idf of 0.45828213102352455.\n",
      "     The word 450             in the highlight 41080    has a tf-idf of 0.33519519342543935.\n",
      "     The word not             in the highlight 21343    has a tf-idf of 0.19704471329792322.\n",
      "     The word lost            in the highlight 157905   has a tf-idf of 0.2509309262371836.\n",
      "     The word say             in the highlight 215242   has a tf-idf of 0.14844824058130646.\n",
      "\n",
      "SANITY CHECK 3\n",
      "\n",
      "The following highlight sums up the following tokenized article:\n",
      "\n",
      "Highlight:\n",
      "bojan krkic scores twice in 4 1 win over athletic bilbao after his late call up into team\n",
      "\n",
      "Tokenized article:\n",
      "cnn teenager bojan krkic scored twice barcelona overcame pre match loss key striker zlatan ibrahimovic defeat athletic bilbao 4 1 saturday night move three points clear spain ibrahimovic scored goals spanish champions 2 2 draw arsenal midweek ruled second leg champions league quarterfinal home english club picking calf injury warmup according uk press association 19 year old bojan stepped third league start season much changed barca line staked claim place tuesday assured performance big week ahead european champions face second placed real madrid key clash next saturday massive bearing wins la liga title real return top goal difference ahead match winning two scores racing santander sunday barca coach josep guardiola made several changes sixth placed bilbao ended match eight youth team products pitch midfielder yaya toure went injured winger jeffren celebrated call senior team 26th minute opening goal 22 year old netted first league goal season eric abidal cross lionel messi sent recalled france fullback free left bojan made 2 0 five minutes halftime captain carles puyol playing right back absence suspended daniel alves intercepted stray pass picked young forward played spain despite serbian father doubled tally hour mark fierce right foot shot abidal provided ammunition messi made uncharacteristic first half miss close range netted pedro 67th minute cross argentina forward 26th league goal season 35th overall bilbao pulled goal back markel susaeta goalkeeper victor valdes twice denied ander iturraspe prevent repeat lapse saw barca blow two goal lead arsenal managerless sevilla claimed fourth place 3 0 victory lowly tenerife saturday late match strikers fredi kanoute luis fabiano netting first half jose carlos curled 87th minute free kick mallorca go back ahead sevilla draw almeria sunday victory would lift islanders two points clear tenerife left third bottom deep relegation trouble seven points behind closest team santander real zaragoza moved seven points clear relegation zone 2 0 victory home malaga moving one place opponents zaragoza followed last weekend 3 0 upset third placed valencia argentine midfielder leonardo ponzio smashed opening goal halftime chile striker humberto suazo sealed victory volley javier arizmendi header 14 minutes play\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"\\nSANITY CHECK 1\")\n",
    "\n",
    "# Sanity check of counts_articles.\n",
    "\n",
    "number_articles = counts_articles.shape[0]\n",
    "voc_size_articles = counts_articles.shape[1]\n",
    "\n",
    "print(\"\\nHere are 10 tuples (A, W) with the number of times the word W appears in the article A:\")\n",
    "m = '     The article {0:<6} contains {1:<3} time(s) the word {2:<8}'\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    art = 0\n",
    "    word = 0\n",
    "    \n",
    "    while counts_articles[art, word] == 0:\n",
    "        art = random.randint(0, number_articles - 1)\n",
    "        word = random.randint(0, voc_size_articles - 1)\n",
    "        \n",
    "    for w, i in vocabulary_articles.items():\n",
    "        if i == word:\n",
    "            break\n",
    "    print(m.format(art, counts_articles[art, word],  w))\n",
    "\n",
    "\n",
    "print(\"\\nSANITY CHECK 2\")\n",
    "\n",
    "# Sanity check of tfidf_highlights.\n",
    "\n",
    "number_highlights = tfidf_highlights.shape[0]\n",
    "voc_size_highlights = tfidf_highlights.shape[1]\n",
    "\n",
    "print(\"\\nHere are 10 tuples (H, W) with the tfidf of the word W in the highlight H:\")\n",
    "m = '     The word {2:<15} in the highlight {0:<8} has a tf-idf of {1:<8}.'\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    high = 0\n",
    "    word = 0\n",
    "    \n",
    "    while tfidf_highlights[high, word] == 0:\n",
    "        high = random.randint(0, number_highlights - 1)\n",
    "        word = random.randint(0, voc_size_highlights - 1)\n",
    "        \n",
    "    for w, i in vocabulary_highlights.items():\n",
    "        if i == word:\n",
    "            break\n",
    "    print(m.format(high, tfidf_highlights[high, word],  w))\n",
    "\n",
    "\n",
    "print(\"\\nSANITY CHECK 3\")\n",
    "\n",
    "# Sanity check of the (article, highlight) pairs.\n",
    "pair = random.randint(0, len(pairs)-1)\n",
    "print(\"\\nThe following highlight sums up the following tokenized article:\")\n",
    "print(\"\\nHighlight:\")\n",
    "print(\" \".join(tokenized_highlights[pairs[pair][1]]))\n",
    "\n",
    "print(\"\\nTokenized article:\")\n",
    "print(\" \".join(tokenized_articles[pairs[pair][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
